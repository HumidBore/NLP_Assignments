{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sI97CALV0C34",
        "Vf1URJEz-ADo",
        "_0Xhe6fjWvqv",
        "qZUuz-bW_BO3",
        "mzQ6qRPF_86z",
        "b5RNdNYYHYwQ",
        "XCpn4_SskmsG",
        "AOm0QZVUbpnd",
        "tUY_G3VFZ_Pw",
        "GYUIRZW9bPXW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iE-OJaA7wap3"
      },
      "outputs": [],
      "source": [
        "import os, shutil  #  file management\n",
        "import sys \n",
        "import pandas as pd  #  dataframe management\n",
        "import numpy as np  #  data manipulation\n",
        "# useful during debugging (progress bars)\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "from typing import Callable, List, Dict, Tuple, Set\n",
        "\n",
        "#from tensorflow.random import set_seed\n",
        "\n",
        "\n",
        "#fixed seeds to get reproducible results\n",
        "np.random.seed(42)\n",
        "#set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bulding the dataframe"
      ],
      "metadata": {
        "id": "jXlE3OHGzaYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset download and extraction"
      ],
      "metadata": {
        "id": "MQ0PSVGsWZBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "\n",
        "DATASET_NAME = \"dataset.zip\"\n",
        "DATASET_FOLDERNAME = \"Dataset\"\n",
        "DATASET_SUBFOLDER = \"dependency_treebank/\"\n",
        "SPLIT_DISTRIBUTION = [100, 150, 199]\n",
        "\n",
        "working_folder = os.getcwd()\n",
        "\n",
        "print(\"Current working directory: \" + str(working_folder))\n",
        "\n",
        "dataset_folder = os.path.join(os.getcwd(), DATASET_FOLDERNAME)\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"dataset.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "    print(\"Successful extraction\")\n",
        "\n",
        "#update folder to the extracted one\n",
        "dataset_folder = os.path.join(dataset_folder, DATASET_SUBFOLDER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIj22FhCyKoA",
        "outputId": "91d6e8ca-fff1-4cbb-af47-6bb830b1331e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekoJOGSg7CDd",
        "outputId": "e72865e3-caee-4030-a483-a26d7610b14a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Dataset/dependency_treebank/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataframe construction"
      ],
      "metadata": {
        "id": "sI97CALV0C34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_dataset(dataset_folder: str, \n",
        "                   split_dist: list(), ) -> Dict[str,pd.DataFrame]:\n",
        "    \n",
        "    df_dict = {\"train\": pd.DataFrame(columns=['sentence', 'labels']),\n",
        "                \"val\": pd.DataFrame(columns=['sentence', 'labels']),\n",
        "                \"test\":pd.DataFrame(columns=['sentence', 'labels'])}\n",
        "    split = \"\"\n",
        "\n",
        "    for filename in os.listdir(dataset_folder):\n",
        "        file_path = os.path.join(dataset_folder, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path):\n",
        "                with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "                    # read it and extract \n",
        "                    document_number = filename.split(\"_\")[1].split(\".\")[0]\n",
        "                    if int(document_number) <= split_dist[0]:\n",
        "                        split = \"train\"\n",
        "                    elif split_dist[0] < int(document_number) <= split_dist[1]:\n",
        "                        split = \"val\"\n",
        "                    else:\n",
        "                        split = \"test\"\n",
        "\n",
        "                    df_file = pd.read_table(\n",
        "                        file_path, \n",
        "                        delimiter='\\t', \n",
        "                        names=['word', 'label'], \n",
        "                        usecols=[0,1],\n",
        "                        skip_blank_lines=False)\n",
        "                    \n",
        "                    #splitting file content in sentences\n",
        "                    idx = list(df_file.loc[df_file.isnull()['word']].index)\n",
        "                    idx.append(len(df_file))\n",
        "                    prev = 0\n",
        "                    for sep in idx:\n",
        "                        df_sentence = pd.DataFrame({\n",
        "                            'sentence': [df_file['word'][prev:sep].to_list()], \n",
        "                            'labels': [df_file['label'][prev:sep].to_list()]})\n",
        "                        df_dict[split] = pd.concat([df_dict[split], df_sentence], ignore_index=True)\n",
        "                        prev = sep + 1\n",
        "                    \n",
        "        except Exception as e:\n",
        "            print('Failed to process %s. Reason: %s' % (file_path, e))\n",
        "            sys.exit(0)\n",
        "\n",
        "    return df_dict"
      ],
      "metadata": {
        "id": "JpNObbHg0HqV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = encode_dataset(dataset_folder, SPLIT_DISTRIBUTION)"
      ],
      "metadata": {
        "id": "ocXNiUzD5bOv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict[\"train\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FW074bfD54DK",
        "outputId": "c54b28e2-f82d-4bb3-f677-d4714927c234"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  \\\n",
              "0  [Sir, Peter, Walters, ,, 58-year-old, chairman...   \n",
              "1  [Sir, Peter, will, succeed, Sir, John, Milne, ...   \n",
              "2  [The, Life, Insurance, Co., of, Georgia, has, ...   \n",
              "3  [David, Wu, ,, the, company, 's, representativ...   \n",
              "4  [Life, of, Georgia, is, part, of, the, Nationa...   \n",
              "\n",
              "                                              labels  \n",
              "0  [NNP, NNP, NNP, ,, JJ, NN, IN, NNP, NNP, NNP, ...  \n",
              "1  [NNP, NNP, MD, VB, NNP, NNP, NNP, ,, CD, ,, WP...  \n",
              "2  [DT, NNP, NNP, NNP, IN, NNP, VBZ, RB, VBN, DT,...  \n",
              "3  [NNP, NNP, ,, DT, NN, POS, NN, IN, NNP, ,, VBD...  \n",
              "4  [NNP, IN, NNP, VBZ, NN, IN, DT, NNP, NNP, NNP,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-268c8f7c-78a1-4bef-9f94-49aef6437e8b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Sir, Peter, Walters, ,, 58-year-old, chairman...</td>\n",
              "      <td>[NNP, NNP, NNP, ,, JJ, NN, IN, NNP, NNP, NNP, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Sir, Peter, will, succeed, Sir, John, Milne, ...</td>\n",
              "      <td>[NNP, NNP, MD, VB, NNP, NNP, NNP, ,, CD, ,, WP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, Life, Insurance, Co., of, Georgia, has, ...</td>\n",
              "      <td>[DT, NNP, NNP, NNP, IN, NNP, VBZ, RB, VBN, DT,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[David, Wu, ,, the, company, 's, representativ...</td>\n",
              "      <td>[NNP, NNP, ,, DT, NN, POS, NN, IN, NNP, ,, VBD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Life, of, Georgia, is, part, of, the, Nationa...</td>\n",
              "      <td>[NNP, IN, NNP, VBZ, NN, IN, DT, NNP, NNP, NNP,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-268c8f7c-78a1-4bef-9f94-49aef6437e8b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-268c8f7c-78a1-4bef-9f94-49aef6437e8b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-268c8f7c-78a1-4bef-9f94-49aef6437e8b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict[\"val\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kinbChSf56cz",
        "outputId": "15a23027-68b9-4e84-903d-7d57f069583a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  \\\n",
              "0  [Beauty, Takes, Backseat, To, Safety, on, Brid...   \n",
              "1  [EVERYONE, AGREES, that, most, of, the, nation...   \n",
              "2  [But, there, 's, disagreement, over, how, to, ...   \n",
              "3  [Highway, officials, insist, the, ornamental, ...   \n",
              "4  [But, other, people, do, n't, want, to, lose, ...   \n",
              "\n",
              "                                              labels  \n",
              "0                   [NN, VBZ, NN, TO, NNP, IN, NNPS]  \n",
              "1  [NN, VBZ, IN, JJS, IN, DT, NN, POS, JJ, NNS, V...  \n",
              "2         [CC, EX, VBZ, NN, IN, WRB, TO, VB, PRP, .]  \n",
              "3  [NN, NNS, VBP, DT, JJ, NNS, IN, JJR, NNS, VBP,...  \n",
              "4  [CC, JJ, NNS, VBP, RB, VB, TO, VB, DT, NNS, PO...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a6ef394e-13e7-46f2-b86b-682e7a66eea7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Beauty, Takes, Backseat, To, Safety, on, Brid...</td>\n",
              "      <td>[NN, VBZ, NN, TO, NNP, IN, NNPS]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[EVERYONE, AGREES, that, most, of, the, nation...</td>\n",
              "      <td>[NN, VBZ, IN, JJS, IN, DT, NN, POS, JJ, NNS, V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[But, there, 's, disagreement, over, how, to, ...</td>\n",
              "      <td>[CC, EX, VBZ, NN, IN, WRB, TO, VB, PRP, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Highway, officials, insist, the, ornamental, ...</td>\n",
              "      <td>[NN, NNS, VBP, DT, JJ, NNS, IN, JJR, NNS, VBP,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[But, other, people, do, n't, want, to, lose, ...</td>\n",
              "      <td>[CC, JJ, NNS, VBP, RB, VB, TO, VB, DT, NNS, PO...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6ef394e-13e7-46f2-b86b-682e7a66eea7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a6ef394e-13e7-46f2-b86b-682e7a66eea7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a6ef394e-13e7-46f2-b86b-682e7a66eea7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict[\"test\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "D7U5NfKo57x6",
        "outputId": "dd46b0c7-381c-44d3-b1c7-fae7c49507b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  \\\n",
              "0  [John, F., Barrett, ,, 40, ,, formerly, execut...   \n",
              "1  [Two, leading, constitutional-law, experts, sa...   \n",
              "2  [Professors, Philip, Kurland, of, the, Univers...   \n",
              "3  [A, line-item, veto, is, a, procedure, that, w...   \n",
              "4  [Mr., Bush, has, said, he, would, like, to, be...   \n",
              "\n",
              "                                              labels  \n",
              "0  [NNP, NNP, NNP, ,, CD, ,, RB, JJ, NN, NN, CC, ...  \n",
              "1  [CD, VBG, NN, NNS, VBD, NNP, NNP, VBZ, RB, VB,...  \n",
              "2  [NNP, NNP, NNP, IN, DT, NNP, IN, NNP, CC, NNP,...  \n",
              "3  [DT, JJ, NN, VBZ, DT, NN, WDT, MD, VB, DT, NN,...  \n",
              "4  [NNP, NNP, VBZ, VBN, PRP, MD, VB, TO, VB, JJ, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ddd7ac0-0819-427b-98e7-de3dce119182\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[John, F., Barrett, ,, 40, ,, formerly, execut...</td>\n",
              "      <td>[NNP, NNP, NNP, ,, CD, ,, RB, JJ, NN, NN, CC, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Two, leading, constitutional-law, experts, sa...</td>\n",
              "      <td>[CD, VBG, NN, NNS, VBD, NNP, NNP, VBZ, RB, VB,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Professors, Philip, Kurland, of, the, Univers...</td>\n",
              "      <td>[NNP, NNP, NNP, IN, DT, NNP, IN, NNP, CC, NNP,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[A, line-item, veto, is, a, procedure, that, w...</td>\n",
              "      <td>[DT, JJ, NN, VBZ, DT, NN, WDT, MD, VB, DT, NN,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Mr., Bush, has, said, he, would, like, to, be...</td>\n",
              "      <td>[NNP, NNP, VBZ, VBN, PRP, MD, VB, TO, VB, JJ, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ddd7ac0-0819-427b-98e7-de3dce119182')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ddd7ac0-0819-427b-98e7-de3dce119182 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ddd7ac0-0819-427b-98e7-de3dce119182');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training data\n",
        "x_train = df_dict['train']['sentence']\n",
        "y_train = df_dict['train']['labels']\n",
        "\n",
        "#Validation data\n",
        "x_val = df_dict['val']['sentence']\n",
        "y_val = df_dict['val']['labels']\n",
        "\n",
        "#Test data\n",
        "x_test = df_dict['test']['sentence']\n",
        "y_test = df_dict['test']['labels']"
      ],
      "metadata": {
        "id": "e2i-qWdRdjV-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glove Embedding model, vocabulary and OOV detection"
      ],
      "metadata": {
        "id": "Vf1URJEz-ADo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Glove embedding"
      ],
      "metadata": {
        "id": "_0Xhe6fjWvqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "def load_GloVe_embedding(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model"
      ],
      "metadata": {
        "id": "S5nCta0f9_X0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIMENSION = 50\n",
        "glove_emb_model = load_GloVe_embedding(EMBEDDING_DIMENSION)"
      ],
      "metadata": {
        "id": "afdQh5i8CDsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe174cf-dbba-4e90-ceec-4f2ca4545d9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Vocabulary"
      ],
      "metadata": {
        "id": "qZUuz-bW_BO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def build_vocabulary(sr: pd.Series) -> List[str]:\n",
        "    \"\"\"\n",
        "    Given a dataset, builds the corresponding word vocabulary.\n",
        "\n",
        "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
        "    :return:\n",
        "      - vocabulary: set of unique terms that build up the vocabulary\n",
        "    \"\"\"\n",
        "    vocabulary = []\n",
        "    for sentence in tqdm(sr):\n",
        "        for token in sentence:\n",
        "            if token not in vocabulary:\n",
        "                vocabulary.append(token)\n",
        "\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "l98tkgAW_DGT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_dict = {}\n",
        "for split in df_dict.keys():\n",
        "    vocabulary_dict[split] = build_vocabulary(x_train)\n",
        "    print()\n",
        "    print(f'[Debug] {split} vocabulary size: {len(vocabulary_dict[split])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D37IdA4AeHo",
        "outputId": "b3e0ec56-d9b6-4342-9fb6-f6634f151686"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1963/1963 [00:01<00:00, 1556.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] train vocabulary size: 8009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1963/1963 [00:01<00:00, 1556.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] val vocabulary size: 8009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1963/1963 [00:01<00:00, 1567.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] test vocabulary size: 8009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOV detection"
      ],
      "metadata": {
        "id": "mzQ6qRPF_86z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_OOV_terms(vocabulary: List[str],\n",
        "                    word_listing: List[str]):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    embedding_vocabulary = set(vocabulary)\n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)"
      ],
      "metadata": {
        "id": "hvm7cbv1BYtq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OOV1 = check_OOV_terms(glove_emb_model.vocab.keys(), vocabulary_dict[\"train\"])\n",
        "OOV1_percentage = float(len(OOV1)) * 100 / len(vocabulary_dict[\"train\"])\n",
        "print(f\"Total OOV terms: {len(OOV1)} ({OOV1_percentage:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2MnS5ilADqv",
        "outputId": "6f026af3-b057-49a0-9955-b245921ccefe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total OOV terms: 2346 (29.29%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lot of words are OOV simply because they start with capital letter, so we will lower all the words and check again the OOV. Before this section we can insert a graph showing the OOV words"
      ],
      "metadata": {
        "id": "zPySrACEC5D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OOV1_lowercase = check_OOV_terms(glove_emb_model.vocab.keys(), [v.lower() for v in vocabulary_dict[\"train\"]])\n",
        "OOV1_lowercase_percentage = float(len(OOV1_lowercase)) * 100 / len(vocabulary_dict[\"train\"])\n",
        "print(f\"Total OOV terms: {len(OOV1_lowercase)} ({OOV1_lowercase_percentage:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB364Xx_C2vo",
        "outputId": "a280d53f-a14e-42a7-ab13-e4889c91ac59"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total OOV terms: 359 (4.48%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\".join(OOV1_lowercase))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWzR_D1dGI-u",
        "outputId": "d840eef3-35af-4f77-faf7-fb37b8238591"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\\/8\n",
            "12,252\n",
            "62%-owned\n",
            "retin-a\n",
            "3057\n",
            "side-crash\n",
            "crocidolite\n",
            "makato\n",
            "program-trading\n",
            "collective-bargaining\n",
            "1\\/4\n",
            "northy\n",
            "synergistics\n",
            "year-ago\n",
            "top-yielding\n",
            "teacher-cadet\n",
            "sub-segments\n",
            "c.j.b.\n",
            "nipponese\n",
            "forest-product\n",
            "security-type\n",
            "coche-dury\n",
            "234.4\n",
            "high-balance\n",
            "wheeland\n",
            "automotive-parts\n",
            "search-and-seizure\n",
            "pattenden\n",
            "sticker-shock\n",
            "ratners\n",
            "six-bottle\n",
            "besuboru\n",
            "rexinger\n",
            "built-from-kit\n",
            "war-rationed\n",
            "4,393,237\n",
            "church-goers\n",
            "sacramento-based\n",
            "9,118\n",
            "foreign-stock\n",
            "achievement-test\n",
            "415.8\n",
            "lap-shoulder\n",
            "3,288,453\n",
            "meinders\n",
            "index-related\n",
            "computer-driven\n",
            "machine-gun-toting\n",
            "college-bowl\n",
            "product-design\n",
            "382-37\n",
            "bumkins\n",
            "energy-services\n",
            "1.5755\n",
            "high-rate\n",
            "one-yen\n",
            "90-cent-an-hour\n",
            "shirt-sleeved\n",
            "374.20\n",
            "pre-1917\n",
            "gingl\n",
            "government-certified\n",
            "jalaalwalikraam\n",
            "autions\n",
            "stock-index\n",
            "life-insurance\n",
            "we-japanese\n",
            "rate-sensitive\n",
            "yen-support\n",
            "three-lawyer\n",
            "two-sevenths\n",
            "446.62\n",
            "direct-investment\n",
            "test-prep\n",
            "twindam\n",
            "corton-charlemagne\n",
            "revenue-desperate\n",
            "video-viewing\n",
            "front-seat\n",
            "-lrb-\n",
            "anti-china\n",
            "durable-goods\n",
            "eight-count\n",
            "odd-sounding\n",
            "romanee-conti\n",
            "beer-belly\n",
            "small-company\n",
            "1.457\n",
            "intellectual-property\n",
            "superpremiums\n",
            "social-studies\n",
            "satrum\n",
            "mehrens\n",
            "-rrb-\n",
            "solaia\n",
            "497.34\n",
            "school-board\n",
            "95,142\n",
            "stirlen\n",
            "landonne\n",
            "market-share\n",
            "nesb\n",
            "16,072\n",
            "forest-products\n",
            "127.03\n",
            "test-preparation\n",
            "1.8415\n",
            "sometimes-exhausting\n",
            "dollar-yen\n",
            "3.253\n",
            "times-stock\n",
            "uzi-model\n",
            "delwin\n",
            "13,056\n",
            "jerritts\n",
            "sharedata\n",
            "red-flag\n",
            "1\\/2\n",
            "271,124\n",
            "mininum-wage\n",
            "prize-fighter\n",
            "industrial-production\n",
            "5.276\n",
            "fetal-tissue\n",
            "food-shop\n",
            "circuit-breaker\n",
            "8300s\n",
            "sino-u.s.\n",
            "morale-damaging\n",
            "yeargin\n",
            "family-planning\n",
            "dust-up\n",
            "car-safety\n",
            "merger-related\n",
            "rapanelli\n",
            "micronite\n",
            "-rcb-\n",
            "post-hearing\n",
            "352.7\n",
            "three-sevenths\n",
            "bermuda-based\n",
            "rubinfien\n",
            "ghkm\n",
            "red-blooded\n",
            "142.85\n",
            "deposits-a\n",
            "junk-bond\n",
            "secilia\n",
            "when-issued\n",
            "test-practice\n",
            "82,389\n",
            "erbamont\n",
            "cray-3\n",
            "equal-opportunity\n",
            "unfair-trade\n",
            "436.01\n",
            "automotive-lighting\n",
            "2645.90\n",
            "anti-takeover\n",
            "identity-management\n",
            "-lcb-\n",
            "asset-sale\n",
            "trettien\n",
            "ac-130u\n",
            "dead-eyed\n",
            "bell-ringer\n",
            "11,762\n",
            "subindustry\n",
            "vinken\n",
            "twin-jet\n",
            "capital-gains\n",
            "one-upsmanship\n",
            "centerbank\n",
            "nagymaros\n",
            "69-point\n",
            "sogo-shosha\n",
            "84-month\n",
            "705.6\n",
            "520-lawyer\n",
            "savings-and-loan\n",
            "highest-pitched\n",
            "derel\n",
            "tiphook\n",
            "auto-safety\n",
            "subskill\n",
            "stock-manipulation\n",
            "limited-partnership\n",
            "macmillan\\/mcgraw-hill\n",
            "lafite-rothschild\n",
            "737.5\n",
            "savers\\/investors\n",
            "higher-salaried\n",
            "colonsville\n",
            "bridgestone\\/firestone\n",
            "pre-1933\n",
            "rope-sight\n",
            "abortion-related\n",
            "wine-buying\n",
            "macheski\n",
            "236.74\n",
            "18,444\n",
            "100,980\n",
            "ntg\n",
            "143.93\n",
            "six-packs\n",
            "money-market\n",
            "more-efficient\n",
            "recession-inspired\n",
            "replacement-car\n",
            "electric-utility\n",
            "3\\/4\n",
            "456.64\n",
            "subminimum\n",
            "449.04\n",
            "futures-related\n",
            "2,303,328\n",
            "4.898\n",
            "purhasing\n",
            "361,376\n",
            "creator's\n",
            "investor-relations\n",
            "143.80\n",
            "purepac\n",
            "bald-faced\n",
            "malizia\n",
            "money-fund\n",
            "antitrust-law\n",
            "wheel-loader\n",
            "change-ringing\n",
            "school-improvement\n",
            "anti-abortionists\n",
            "ensrud\n",
            "hallwood\n",
            "chong-sik\n",
            "flightiness\n",
            "non-biodegradable\n",
            "tarwhine\n",
            "money-center\n",
            "index-options\n",
            "moleculon\n",
            "415.6\n",
            "sanderoff\n",
            "lower-priority\n",
            "less-serious\n",
            "one-country\n",
            "236.79\n",
            "senate-house\n",
            "mortgage-based\n",
            "bellringers\n",
            "low-ability\n",
            "nissho-iwai\n",
            "boorse\n",
            "143.08\n",
            "detective-story\n",
            "kalipharma\n",
            "school-research\n",
            "securities-based\n",
            "incentive-bonus\n",
            "chafic\n",
            "floating-rate\n",
            "telephone-information\n",
            "weisfield\n",
            "low-ball\n",
            "subskills\n",
            "new-home\n",
            "foreign-led\n",
            "62.625\n",
            "drobnick\n",
            "light-truck\n",
            "yen-denominated\n",
            "old-house\n",
            "nih-appointed\n",
            "test-coaching\n",
            "safe-deposit\n",
            "chinchon\n",
            "38.375\n",
            "macmillan\\/mcgraw\n",
            "chemplus\n",
            "akerfeldt\n",
            "buttoned-down\n",
            "374.19\n",
            "year-earlier\n",
            "re-thought\n",
            "samnick\n",
            "ft-se\n",
            "thin-lipped\n",
            "muscolina\n",
            "monchecourt\n",
            "wtd\n",
            "training-wage\n",
            "amphobiles\n",
            "278.7\n",
            "pianist-comedian\n",
            "pennview\n",
            "biondi-santi\n",
            "7\\/8\n",
            "nearly-30\n",
            "co-developers\n",
            "index-arbitrage\n",
            "230-215\n",
            "500,004\n",
            "non-encapsulating\n",
            "tissue-transplant\n",
            "70-a-share\n",
            "state-supervised\n",
            "sport-utility\n",
            "pro-forma\n",
            "nekoosa\n",
            "vitulli\n",
            "unenticing\n",
            "veselich\n",
            "alurralde\n",
            "c-90\n",
            "iran\\/contra\n",
            "cash-rich\n",
            "16.125\n",
            "home-market\n",
            "ctbs\n",
            "30,841\n",
            "greenmailer\n",
            "preparatives\n",
            "glenham\n",
            "ingersoll-rand\n",
            "school-district\n",
            "pramual\n",
            "37-a-share\n",
            "norwick\n",
            "sometimes-tawdry\n",
            "building-products\n",
            "14,821\n",
            "chilver\n",
            "custom-chip\n",
            "ariail\n",
            "language-housekeeper\n",
            "drag-down\n",
            "pathlogy\n",
            "mouth-up\n",
            "cotran\n",
            "integra-a\n",
            "sub-markets\n",
            "big-ticket\n",
            "polyproplene\n",
            "aslacton\n",
            "cop-killer\n",
            "marketing-communications\n",
            "trading-company\n",
            "lezovich\n",
            "1\\/10th\n",
            "student-test\n",
            "trockenbeerenauslesen\n",
            "roof-crush\n",
            "computer-system-design\n",
            "summer\\/winter\n",
            "univest\n",
            "hummerstone\n",
            "water-authority\n",
            "page-one\n",
            "wfrr\n",
            "sell-offs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "Here we replace bracket value with their symbols: \n",
        "```\n",
        "  -lrb- and -lcb-   -->  ( \n",
        "  -rrb- and -rcb-   -->  )\n",
        "```\n",
        "\n",
        "In addition, all the rational numbers will be replaced with the placeholder #number#, as long as the floating point numbers.\n",
        "Note that rational numbers, instead of being like 3/4, are written as 3\\/4. The cause is that symbol \"/\" is represented using \"\\/\", as this happens also in other words that are notrational ones\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b5RNdNYYHYwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(content_list: List[str]) -> List[str]:\n",
        "    placeholder = \"#number#\"\n",
        "    re_slashes = re.compile('\\\\\\/')  #pattern \\/ \n",
        "    re_rational = re.compile('\\d+\\/\\d+')  #pattern rational number (e.g. 1/5)\n",
        "    re_number = re.compile('[+-]?(\\d*[.])?\\d+')  #pattern decimal number (e.g. 3.14)\n",
        "    re_left_bracket = re.compile('(-lrb-)|(-lcb-)')  #pattern left bracket\n",
        "    re_right_bracket = re.compile('(-rrb-)|(-rcb-)')  #pattern right bracket\n",
        "    re_slashed_words = re.compile(\"(\\w*)\\/(\\w*)\")  #a slash separating words will be replaced with a dash, following the trend of the dataset, where composed words are in the form word-word\n",
        "\n",
        "    content_list_preprocessed = [content.lower() for content in content_list]\n",
        "    content_list_preprocessed = [re_slashes.sub(\"/\", content) for content in content_list_preprocessed]\n",
        "    content_list_preprocessed = [re_left_bracket.sub(\"(\", content) for content in content_list_preprocessed]\n",
        "    content_list_preprocessed = [re_right_bracket.sub(\")\", content) for content in content_list_preprocessed]\n",
        "    content_list_preprocessed = [placeholder if re.match(re_rational, content) else content for content in content_list_preprocessed]\n",
        "    content_list_preprocessed = [placeholder if re.match(re_number, content) else content for content in content_list_preprocessed]\n",
        "    content_list_preprocessed = [content.replace(\"/\", \"-\") if re.match(re_slashed_words, content) else content for content in content_list_preprocessed]\n",
        "\n",
        "    return content_list_preprocessed\n"
      ],
      "metadata": {
        "id": "d9H61yyiHYbT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the dataset"
      ],
      "metadata": {
        "id": "OzNl0nSGdEwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_preprocessed = x_train.apply(preprocessing)"
      ],
      "metadata": {
        "id": "GyjnI2JZbADI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the new vocabulary after preprocessing"
      ],
      "metadata": {
        "id": "M-Vw8s63garq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_vocabulary_preprocessed = build_vocabulary(x_train_preprocessed)\n",
        "print()\n",
        "print(f'[Debug] train vocabulary size after preprocessing: {len(train_vocabulary_preprocessed)}')\n",
        "\n",
        "OOV1_preprocessed = check_OOV_terms(glove_emb_model.vocab.keys(), train_vocabulary_preprocessed)\n",
        "OOV1_preprocessed_percentage = float(len(OOV1_preprocessed)) * 100 / len(train_vocabulary_preprocessed)\n",
        "print(f\"Total OOV terms: {len(OOV1_preprocessed)} ({OOV1_preprocessed_percentage:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcxlVwQFgaRr",
        "outputId": "86ce8677-8d7c-4643-ac2f-bfd242e790af"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1963/1963 [00:01<00:00, 1796.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] train vocabulary size after preprocessing: 6919\n",
            "Total OOV terms: 290 (4.19%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that the number of OOV words has plummetted with respect to the the non preprocessed data. Similarly to train data, we apply preprocessing to validation and test splits."
      ],
      "metadata": {
        "id": "j5OLPRGc6svj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val_preprocessed = x_val.apply(preprocessing)\n",
        "x_test_preprocessed = x_test.apply(preprocessing)"
      ],
      "metadata": {
        "id": "MEec69AT_KEw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping\n",
        "Since we want to work with numerical data only, we will mapp words and pos (labels) to numbers. "
      ],
      "metadata": {
        "id": "EElOhl9F-x9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Methods to create mapping\n",
        "\n",
        "#adds oov words at the end of vocabulary\n",
        "def extend_vocabulary(word_to_idx_original: Dict[str, int],\n",
        "                      words_to_add) -> Tuple[Dict[str, int],Dict[int, str]]:\n",
        "  \"\"\"\n",
        "    Given mapping between word and indeces, adds new words.\n",
        "\n",
        "    :param word_to_idx_original: dictionary with key=word and value=index to which the word is mapped\n",
        "    :return:\n",
        "      - word_to_idx_extended: word_to_idx with new words\n",
        "      - idx_to_word_extended: swapped version of word_to_idx_extended (keys and values are swapped)\n",
        "  \"\"\"\n",
        "  word_to_idx_extended = word_to_idx_original\n",
        "  idx = len(word_to_idx_extended.keys())\n",
        "  if idx == 0: \n",
        "    idx = 1  #position 0 is reserved\n",
        "\n",
        "  for sentence in words_to_add:\n",
        "      for token in sentence:\n",
        "          if token not in word_to_idx_extended:\n",
        "              word_to_idx_extended[token] = idx \n",
        "              idx += 1\n",
        "  idx_to_word_extended = {v: k for k, v in word_to_idx_extended.items()}\n",
        "\n",
        "  return word_to_idx_extended, idx_to_word_extended\n",
        "\n",
        "def encode_into_numbers(sentences: List[str],\n",
        "                        word_to_idx_mapping: Dict[str, int]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Return a list of sequences encoded into integers following the mapping of the vocabulary\n",
        "    \"\"\"\n",
        "    encoded_data = [[word_to_idx_mapping[token] for token in sentence] for sentence in sentences]\n",
        " \n",
        "    return encoded_data\n",
        "\n",
        "def decode_into_words(encoded_sentences: List[str],\n",
        "                        idx_to_word_mapping: Dict[int,str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Return a list of sequences decoded back to words following the (reverse) mapping of the vocabulary\n",
        "    \"\"\"\n",
        "    decoded_data = [[idx_to_word_mapping[index] for index in sentence] for sentence in encoded_sentences]\n",
        " \n",
        "    return decoded_data\n"
      ],
      "metadata": {
        "id": "_j91cRBqAN1x"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating vocabulary mapping for the words in the data set\n",
        "#Note that they are incremental, this means that the val vocabulary includes the rain one, and the test one inlcudes train and val ones\n",
        "#This has been made according to the guidelines on the construction of V1, V2, V3, V4. \n",
        "#All in all, the complete vocabulary is the one with _test suffix\n",
        "#In the embedding section the intermediate vocabularies will be used according to what they contain. For example, to compute the embedding matrix on the train set, we will use word_to_idx_train, while for validation word_to_idx_val\n",
        "word_to_idx_train, idx_to_word_train = extend_vocabulary({}, [glove_emb_model.vocab.keys()] + x_train_preprocessed.tolist())\n",
        "print(\"Train vocabulary size: \", len(word_to_idx_train))\n",
        "word_to_idx_val, idx_to_word_val = extend_vocabulary(word_to_idx_train, x_val_preprocessed.tolist())\n",
        "print(\"Val vocabulary size: \", len(word_to_idx_train))\n",
        "word_to_idx_test, idx_to_word_test = extend_vocabulary(word_to_idx_val, x_test_preprocessed.tolist())\n",
        "print(\"Test vocabulary size: \", len(word_to_idx_test))\n",
        "\n",
        "#encoding the data set\n",
        "x_train_enc = encode_into_numbers(x_train_preprocessed.tolist(), word_to_idx_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbeQJNAlEzl2",
        "outputId": "291703c9-5435-4c31-d8d1-43a119daa491"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train vocabulary size:  400290\n",
            "Val vocabulary size:  400431\n",
            "Test vocabulary size:  400503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating vocabulary mapping for the labels in the whole dataset\n",
        "label_to_idx, idx_to_label = extend_vocabulary({},  y_train.tolist() + y_val.tolist() + y_test.tolist())\n",
        "\n",
        "y_train_enc = encode_into_numbers(y_train.tolist(), label_to_idx)\n",
        "y_val_enc = encode_into_numbers(y_val.tolist(), label_to_idx)\n",
        "y_test_enc = encode_into_numbers(y_test.tolist(), label_to_idx)\n",
        "\n",
        "number_pos = len(label_to_idx)\n",
        "print(f\"In the dataset there are {number_pos} distinct POS\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PzlKA-eHJBr",
        "outputId": "4a78fac4-20f2-49a5-84de-dff4e568cba1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the dataset there are 45 distinct POS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding matrix"
      ],
      "metadata": {
        "id": "mAv5AHB_REM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dashed_embeddings(embedding_model, word):\n",
        "  if word.contains(\"-\"):\n",
        "    words_split = word.split(\"-\").sort(key=len)  #getting the encoding of compound words starting from the longest one\n",
        "    for word_piece in words_split:\n",
        "      try:\n",
        "        return embedding_model[word]  #if a word is found, assign its embedding to the matrix element\n",
        "      except:\n",
        "        pass  #if a word is not found, do nothing\n",
        "  return None\n",
        "\n",
        "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                           embedding_dimension: int,\n",
        "                           word_to_idx: Dict[str, int]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param embedding_dimension: \n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension), dtype=np.float32)\n",
        "    \n",
        "    #adding all GloVe vocabularies embeddings\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "      if word in embedding_model.keys():\n",
        "          embedding_matrix[idx] = embedding_model[word]\n",
        "      elif word.contains(\"-\"): \n",
        "          dashed_embedding = get_dashed_embeddings(embedding_model, word)\n",
        "          if dashed_embedding is None: #it means that word has no dash or all its subwords are oov\n",
        "              embedding_matrix[idx] = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        " \n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "#FUNCTION TO BE FINISHED\n",
        "def extend_embedding_matrix(embedding_matrix: np.ndarray,\n",
        "                            word_to_idx: Dict[str, int]) -> np.ndarray:\n",
        "\n",
        "    oov_embedding_matrix = np.zeros((len(oov_terms), embedding_matrix.shape[1]), dtype=np.float32)\n",
        "    for idx, oov in enumerate(oov_terms):\n",
        "        embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_matrix.shape[1])\n",
        "        oov_embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    new_embedding_matrix = np.concatenate([embedding_matrix, oov_embedding_matrix])\n",
        "\n",
        "    return new_embedding_matrix"
      ],
      "metadata": {
        "id": "QGCi8pguRH-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # The folllowing section is only for reference, as it has been completely rewritten"
      ],
      "metadata": {
        "id": "XCpn4_SskmsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OOV handling"
      ],
      "metadata": {
        "id": "AOm0QZVUbpnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this function extends the current embedding matrix with the embeddings of the oov terms\n",
        "def extend_embedding_matrix(embedding_matrix: np.ndarray,\n",
        "                            oov_terms: List[str]) -> np.ndarray:\n",
        "\n",
        "    oov_embedding_matrix = np.zeros((len(oov_terms), embedding_matrix.shape[1]), dtype=np.float32)\n",
        "    for idx, oov in enumerate(oov_terms):\n",
        "        embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_matrix.shape[1])\n",
        "        oov_embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    new_embedding_matrix = np.concatenate([embedding_matrix, oov_embedding_matrix])\n",
        "\n",
        "    return new_embedding_matrix"
      ],
      "metadata": {
        "id": "ClM447bcOIKn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check OOV1 (oov in the train set) and add OOV1 embeddings to the matrix"
      ],
      "metadata": {
        "id": "i5zlr_imBZMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OOV1 = check_OOV_terms(set(glove_emb_model.vocab.keys()), word_listing_train)"
      ],
      "metadata": {
        "id": "yxWpJouSC3ZA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "6c1d27d6-93ad-43fb-c279-ee1b2b973342"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-49932e0aeb81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mOOV1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_OOV_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_emb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_listing_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'word_listing_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oov_percentage = float(len(OOV1)) * 100 / len(word_listing_train)\n",
        "print(f\"Total OOV terms: {len(OOV1)} ({oov_percentage:.2f}%)\")"
      ],
      "metadata": {
        "id": "8j0KBHdXDTf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding OOV1 embeddings to the matrix"
      ],
      "metadata": {
        "id": "mPpBxdhsVolP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = extend_embedding_matrix(embedding_matrix, OOV1)\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
      ],
      "metadata": {
        "id": "xqSqEagMTGK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check OOV2 (oov in the validation set) and add OOV2 embeddings to the matrix"
      ],
      "metadata": {
        "id": "tUY_G3VFZ_Pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OOV2 = check_OOV_terms(set(glove_emb_model.vocab.keys()).union(set(OOV1)), word_listing_val)"
      ],
      "metadata": {
        "id": "icE_iBCjZ_P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov2_percentage = float(len(OOV2)) * 100 / len(word_listing_val)\n",
        "print(f\"Total OOV terms: {len(OOV2)} ({oov2_percentage:.2f}%)\")"
      ],
      "metadata": {
        "id": "BQeFn7fQZ_P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Adding OOV2 embeddings to the matrix\n",
        "\n",
        "This cell is useless because OOV2 is empty"
      ],
      "metadata": {
        "id": "8j5sjYhmZ_P3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = extend_embedding_matrix(embedding_matrix, OOV2)\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
      ],
      "metadata": {
        "id": "OCwm_WIwZ_P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check OOV3 (oov in the test set) and add OOV3 embeddings to the matrix"
      ],
      "metadata": {
        "id": "GYUIRZW9bPXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OOV3 = check_OOV_terms(set(glove_emb_model.vocab.keys()).union(set(OOV1)).union(set(OOV2)), word_listing_test)"
      ],
      "metadata": {
        "id": "fk8W0p48bPXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov3_percentage = float(len(OOV3)) * 100 / len(word_listing_test)\n",
        "print(f\"Total OOV terms: {len(OOV3)} ({oov3_percentage:.2f}%)\")"
      ],
      "metadata": {
        "id": "8rVI78EFbPXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Adding OOV3 embeddings to the matrix\n",
        "This cell is useless too, as no oov term is present in the test set"
      ],
      "metadata": {
        "id": "WkEY-YJ6bPXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = extend_embedding_matrix(embedding_matrix, OOV3)\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
      ],
      "metadata": {
        "id": "BInrt9tJbPXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build embedding matrix"
      ],
      "metadata": {
        "id": "M-T50oWZXHTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting from GloVe matrix"
      ],
      "metadata": {
        "id": "uXu5FElyFHXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                           embedding_dimension: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param embedding_dimension: \n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((len(embedding_model.index2word), embedding_dimension), dtype=np.float32)\n",
        "    \n",
        "    #adding all GloVe vocabularies embeddings\n",
        "    for idx, word in enumerate(embedding_model.index2word):\n",
        "         embedding_vector = embedding_model[word]\n",
        "         embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "Etd4xRC2Qi15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vocab_size = len(glove_emb_model.index2word) + len(OOV1)\n",
        "embedding_matrix = build_embedding_matrix(glove_emb_model, EMBEDDING_DIMENSION)\n",
        "print()\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
      ],
      "metadata": {
        "id": "mmzp9NYFQ7kD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}