{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MQ0PSVGsWZBW",
        "sI97CALV0C34",
        "_0Xhe6fjWvqv",
        "qZUuz-bW_BO3",
        "mzQ6qRPF_86z",
        "b5RNdNYYHYwQ",
        "XCpn4_SskmsG",
        "tUY_G3VFZ_Pw",
        "GYUIRZW9bPXW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iE-OJaA7wap3"
      },
      "outputs": [],
      "source": [
        "#General imports\n",
        "import copy\n",
        "import os  \n",
        "import pandas as pd  #  dataframe management\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np  #  data manipulation\n",
        "import re\n",
        "import sys \n",
        "from tqdm import tqdm\n",
        "from typing import Callable, List, Dict, Tuple, Set\n",
        "\n",
        "#tensorflow imports\n",
        "from tensorflow.keras.layers import Bidirectional,  Dense, Dropout, Embedding, GRU, Input, LSTM, TimeDistributed\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "\n",
        "#fixed seeds to get reproducible results\n",
        "np.random.seed(42)\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bulding the dataframe"
      ],
      "metadata": {
        "id": "jXlE3OHGzaYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset download and extraction"
      ],
      "metadata": {
        "id": "MQ0PSVGsWZBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request  #  download files\n",
        "import zipfile  #  unzip files\n",
        "\n",
        "DATASET_NAME = \"dataset.zip\"\n",
        "DATASET_FOLDERNAME = \"Dataset\"\n",
        "DATASET_SUBFOLDER = \"dependency_treebank/\"\n",
        "SPLIT_DISTRIBUTION = [100, 150, 199]\n",
        "\n",
        "working_folder = os.getcwd()\n",
        "\n",
        "print(\"Current working directory: \" + str(working_folder))\n",
        "\n",
        "dataset_folder = os.path.join(os.getcwd(), DATASET_FOLDERNAME)\n",
        "\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip'\n",
        "\n",
        "dataset_path = os.path.join(dataset_folder, \"dataset.zip\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "    print(\"Successful download\")\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_folder)\n",
        "    print(\"Successful extraction\")\n",
        "\n",
        "#update folder to the extracted one\n",
        "dataset_folder = os.path.join(dataset_folder, DATASET_SUBFOLDER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIj22FhCyKoA",
        "outputId": "984cb763-97bb-40b6-976c-35fd87c31c7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "Successful download\n",
            "Successful extraction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekoJOGSg7CDd",
        "outputId": "68e30f88-9bd2-4d01-9aaf-77f4c5955f70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Dataset/dependency_treebank/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataframe construction"
      ],
      "metadata": {
        "id": "sI97CALV0C34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_dataset(dataset_folder: str, \n",
        "                   split_dist: list(), ) -> Dict[str,pd.DataFrame]:\n",
        "    \n",
        "    df_dict = {\"train\": pd.DataFrame(columns=['sentence', 'labels']),\n",
        "                \"val\": pd.DataFrame(columns=['sentence', 'labels']),\n",
        "                \"test\":pd.DataFrame(columns=['sentence', 'labels'])}\n",
        "    split = \"\"\n",
        "\n",
        "    for filename in sorted(os.listdir(dataset_folder)):\n",
        "        file_path = os.path.join(dataset_folder, filename)\n",
        "        try:\n",
        "            if os.path.isfile(file_path):\n",
        "                with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
        "                    # read it and extract \n",
        "                    document_number = filename.split(\"_\")[1].split(\".\")[0]\n",
        "                    if int(document_number) <= split_dist[0]:\n",
        "                        split = \"train\"\n",
        "                    elif split_dist[0] < int(document_number) <= split_dist[1]:\n",
        "                        split = \"val\"\n",
        "                    else:\n",
        "                        split = \"test\"\n",
        "\n",
        "                    df_file = pd.read_table(\n",
        "                        file_path, \n",
        "                        delimiter='\\t', \n",
        "                        names=['word', 'label'], \n",
        "                        usecols=[0,1],\n",
        "                        skip_blank_lines=False)\n",
        "                    \n",
        "                    #splitting file content in sentences\n",
        "                    idx = list(df_file.loc[df_file.isnull()['word']].index)\n",
        "                    idx.append(len(df_file))\n",
        "                    prev = 0\n",
        "                    for sep in idx:\n",
        "                        df_sentence = pd.DataFrame({\n",
        "                            'sentence': [df_file['word'][prev:sep].to_list()], \n",
        "                            'labels': [df_file['label'][prev:sep].to_list()]})\n",
        "                        df_dict[split] = pd.concat([df_dict[split], df_sentence], ignore_index=True)\n",
        "                        prev = sep + 1\n",
        "                    \n",
        "        except Exception as e:\n",
        "            print('Failed to process %s. Reason: %s' % (file_path, e))\n",
        "            sys.exit(0)\n",
        "\n",
        "    return df_dict"
      ],
      "metadata": {
        "id": "JpNObbHg0HqV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = encode_dataset(dataset_folder, SPLIT_DISTRIBUTION)"
      ],
      "metadata": {
        "id": "ocXNiUzD5bOv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict[\"train\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FW074bfD54DK",
        "outputId": "82f46b08-f24f-4bf5-b09c-b1a49be952ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  \\\n",
              "0  [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
              "1  [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
              "2  [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
              "3  [A, form, of, asbestos, once, used, to, make, ...   \n",
              "4  [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
              "\n",
              "                                              labels  \n",
              "0  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  \n",
              "1  [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...  \n",
              "2  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  \n",
              "3  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  \n",
              "4  [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-750a27ac-17c8-4c05-b8f7-04812548065b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
              "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
              "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
              "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
              "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-750a27ac-17c8-4c05-b8f7-04812548065b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-750a27ac-17c8-4c05-b8f7-04812548065b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-750a27ac-17c8-4c05-b8f7-04812548065b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict[\"val\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kinbChSf56cz",
        "outputId": "ed57dc63-f60b-412d-9659-4f0ceffa68be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  \\\n",
              "0  [A, House-Senate, conference, approved, major,...   \n",
              "1  [For, the, Agency, for, International, Develop...   \n",
              "2  [The, conference, approved, at, least, $, 55, ...   \n",
              "3  [The, agreement, on, Poland, contrasts, with, ...   \n",
              "4  [These, fiscal, pressures, are, also, a, facto...   \n",
              "\n",
              "                                              labels  \n",
              "0  [DT, NNP, NN, VBD, JJ, NNS, IN, DT, NN, IN, JJ...  \n",
              "1  [IN, DT, NNP, IN, NNP, NNP, ,, NNS, VBD, $, CD...  \n",
              "2  [DT, NN, VBD, IN, JJS, $, CD, CD, IN, JJ, NN, ...  \n",
              "3  [DT, NN, IN, NNP, VBZ, IN, DT, JJ, NNS, VBG, I...  \n",
              "4  [DT, JJ, NNS, VBP, RB, DT, NN, IN, VBG, DT, NN...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd3422b4-a342-492b-a67e-568f8a5218dc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[A, House-Senate, conference, approved, major,...</td>\n",
              "      <td>[DT, NNP, NN, VBD, JJ, NNS, IN, DT, NN, IN, JJ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[For, the, Agency, for, International, Develop...</td>\n",
              "      <td>[IN, DT, NNP, IN, NNP, NNP, ,, NNS, VBD, $, CD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[The, conference, approved, at, least, $, 55, ...</td>\n",
              "      <td>[DT, NN, VBD, IN, JJS, $, CD, CD, IN, JJ, NN, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[The, agreement, on, Poland, contrasts, with, ...</td>\n",
              "      <td>[DT, NN, IN, NNP, VBZ, IN, DT, JJ, NNS, VBG, I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[These, fiscal, pressures, are, also, a, facto...</td>\n",
              "      <td>[DT, JJ, NNS, VBP, RB, DT, NN, IN, VBG, DT, NN...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd3422b4-a342-492b-a67e-568f8a5218dc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dd3422b4-a342-492b-a67e-568f8a5218dc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dd3422b4-a342-492b-a67e-568f8a5218dc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict[\"test\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "D7U5NfKo57x6",
        "outputId": "da24ad87-7034-4566-afb6-cffd8b6fe933"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  \\\n",
              "0  [Intelogic, Trace, Inc., ,, San, Antonio, ,, T...   \n",
              "1  [The, move, boosts, Intelogic, Chairman, Asher...   \n",
              "2  [Mr., Ackerman, already, is, seeking, to, oust...   \n",
              "3  [The, action, followed, by, one, day, an, Inte...   \n",
              "4  [In, New, York, Stock, Exchange, composite, tr...   \n",
              "\n",
              "                                              labels  \n",
              "0  [NNP, NNP, NNP, ,, NNP, NNP, ,, NNP, ,, VBD, P...  \n",
              "1  [DT, NN, VBZ, NNP, NNP, NNP, NNP, POS, NN, TO,...  \n",
              "2  [NNP, NNP, RB, VBZ, VBG, TO, VB, NNP, NNP, IN,...  \n",
              "3  [DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...  \n",
              "4  [IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-efd5341c-4f64-4a3e-b82d-6d81dc9b52bb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Intelogic, Trace, Inc., ,, San, Antonio, ,, T...</td>\n",
              "      <td>[NNP, NNP, NNP, ,, NNP, NNP, ,, NNP, ,, VBD, P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[The, move, boosts, Intelogic, Chairman, Asher...</td>\n",
              "      <td>[DT, NN, VBZ, NNP, NNP, NNP, NNP, POS, NN, TO,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Mr., Ackerman, already, is, seeking, to, oust...</td>\n",
              "      <td>[NNP, NNP, RB, VBZ, VBG, TO, VB, NNP, NNP, IN,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[The, action, followed, by, one, day, an, Inte...</td>\n",
              "      <td>[DT, NN, VBN, IN, CD, NN, DT, NNP, NN, IN, PRP...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[In, New, York, Stock, Exchange, composite, tr...</td>\n",
              "      <td>[IN, NNP, NNP, NNP, NNP, JJ, NN, NN, ,, NNP, N...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efd5341c-4f64-4a3e-b82d-6d81dc9b52bb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-efd5341c-4f64-4a3e-b82d-6d81dc9b52bb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-efd5341c-4f64-4a3e-b82d-6d81dc9b52bb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = {\"train\": df_dict['train']['sentence'],\n",
        "     \"val\": df_dict['val']['sentence'],\n",
        "     \"test\": df_dict['test']['sentence']}\n",
        "\n",
        "y = {\"train\": df_dict['train']['labels'],\n",
        "     \"val\": df_dict['val']['labels'],\n",
        "     \"test\": df_dict['test']['labels']}"
      ],
      "metadata": {
        "id": "e2i-qWdRdjV-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glove Embedding model, vocabulary and OOV detection"
      ],
      "metadata": {
        "id": "Vf1URJEz-ADo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Glove embedding"
      ],
      "metadata": {
        "id": "_0Xhe6fjWvqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "def load_GloVe_embedding(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model"
      ],
      "metadata": {
        "id": "S5nCta0f9_X0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIMENSION = 50\n",
        "glove_emb_model = load_GloVe_embedding(EMBEDDING_DIMENSION)"
      ],
      "metadata": {
        "id": "afdQh5i8CDsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9751b15-c2cc-4423-d657-0c820c3becf6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating initial vocabulary"
      ],
      "metadata": {
        "id": "qZUuz-bW_BO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def build_vocabulary(sr: pd.Series) -> List[str]:\n",
        "    \"\"\"\n",
        "    Given a dataset, builds the corresponding word vocabulary.\n",
        "\n",
        "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
        "    :return:\n",
        "      - vocabulary: set of unique terms that build up the vocabulary\n",
        "    \"\"\"\n",
        "    vocabulary = []\n",
        "    for sentence in tqdm(sr):\n",
        "        for token in sentence:\n",
        "            if token not in vocabulary:\n",
        "                vocabulary.append(token)\n",
        "\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "l98tkgAW_DGT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_dict = {}\n",
        "for split in df_dict.keys():\n",
        "    vocabulary_dict[split] = build_vocabulary(x[split])\n",
        "    print()\n",
        "    print(f'[Debug] {split} vocabulary size: {len(vocabulary_dict[split])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D37IdA4AeHo",
        "outputId": "64cabd67-3c09-4c98-b5d5-c1c4eae2ba62"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1963/1963 [00:01<00:00, 1888.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] train vocabulary size: 8009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1299/1299 [00:00<00:00, 2837.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] val vocabulary size: 5892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 652/652 [00:00<00:00, 4436.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] test vocabulary size: 3623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOV detection"
      ],
      "metadata": {
        "id": "mzQ6qRPF_86z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_OOV_terms(vocabulary: List[str],\n",
        "                    word_listing: List[str]):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    embedding_vocabulary = set(vocabulary)\n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)"
      ],
      "metadata": {
        "id": "hvm7cbv1BYtq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OOV1 = check_OOV_terms(glove_emb_model.vocab.keys(), vocabulary_dict[\"train\"])\n",
        "OOV1_percentage = float(len(OOV1)) * 100 / len(vocabulary_dict[\"train\"])\n",
        "print(f\"Total OOV terms: {len(OOV1)} ({OOV1_percentage:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2MnS5ilADqv",
        "outputId": "08b4ca95-6741-40d1-8f2c-44b66a0d5e0b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total OOV terms: 2346 (29.29%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lot of words are OOV simply because they start with capital letter, so we will lower all the words and check again the OOV. Before this section we can insert a graph showing the OOV words"
      ],
      "metadata": {
        "id": "zPySrACEC5D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OOV1_lowercase = check_OOV_terms(glove_emb_model.vocab.keys(), [v.lower() for v in vocabulary_dict[\"train\"]])\n",
        "OOV1_lowercase_percentage = float(len(OOV1_lowercase)) * 100 / len(vocabulary_dict[\"train\"])\n",
        "print(f\"Total OOV terms: {len(OOV1_lowercase)} ({OOV1_lowercase_percentage:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB364Xx_C2vo",
        "outputId": "b671b055-9491-4eb4-d9f6-c3d563ccc742"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total OOV terms: 359 (4.48%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\".join(OOV1_lowercase))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWzR_D1dGI-u",
        "outputId": "b94e4a07-f44f-4e65-a121-17e54766f7c8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "red-flag\n",
            "2645.90\n",
            "lower-priority\n",
            "one-upsmanship\n",
            "pianist-comedian\n",
            "amphobiles\n",
            "436.01\n",
            "preparatives\n",
            "cray-3\n",
            "higher-salaried\n",
            "prize-fighter\n",
            "1.457\n",
            "bumkins\n",
            "investor-relations\n",
            "purhasing\n",
            "we-japanese\n",
            "samnick\n",
            "30,841\n",
            "flightiness\n",
            "520-lawyer\n",
            "non-encapsulating\n",
            "sticker-shock\n",
            "forest-product\n",
            "bermuda-based\n",
            "collective-bargaining\n",
            "234.4\n",
            "low-ability\n",
            "automotive-parts\n",
            "index-related\n",
            "cotran\n",
            "11,762\n",
            "revenue-desperate\n",
            "new-home\n",
            "built-from-kit\n",
            "236.79\n",
            "ac-130u\n",
            "ensrud\n",
            "mouth-up\n",
            "securities-based\n",
            "wtd\n",
            "solaia\n",
            "savers\\/investors\n",
            "twindam\n",
            "nih-appointed\n",
            "more-efficient\n",
            "landonne\n",
            "floating-rate\n",
            "18,444\n",
            "one-country\n",
            "trockenbeerenauslesen\n",
            "lezovich\n",
            "industrial-production\n",
            "chafic\n",
            "9,118\n",
            "building-products\n",
            "boorse\n",
            "415.8\n",
            "dust-up\n",
            "pre-1917\n",
            "year-ago\n",
            "1\\/4\n",
            "red-blooded\n",
            "market-share\n",
            "macheski\n",
            "autions\n",
            "446.62\n",
            "micronite\n",
            "subindustry\n",
            "index-arbitrage\n",
            "test-coaching\n",
            "anti-takeover\n",
            "computer-driven\n",
            "capital-gains\n",
            "cash-rich\n",
            "antitrust-law\n",
            "year-earlier\n",
            "127.03\n",
            "one-yen\n",
            "16.125\n",
            "3\\/4\n",
            "money-market\n",
            "rope-sight\n",
            "makato\n",
            "unenticing\n",
            "vinken\n",
            "143.93\n",
            "500,004\n",
            "ariail\n",
            "sub-segments\n",
            "chilver\n",
            "sogo-shosha\n",
            "3.253\n",
            "143.08\n",
            "colonsville\n",
            "achievement-test\n",
            "-rcb-\n",
            "cop-killer\n",
            "test-preparation\n",
            "5\\/8\n",
            "eight-count\n",
            "replacement-car\n",
            "secilia\n",
            "sharedata\n",
            "recession-inspired\n",
            "merger-related\n",
            "ntg\n",
            "centerbank\n",
            "subskills\n",
            "synergistics\n",
            "mortgage-based\n",
            "alurralde\n",
            "tarwhine\n",
            "12,252\n",
            "meinders\n",
            "anti-abortionists\n",
            "safe-deposit\n",
            "foreign-led\n",
            "sino-u.s.\n",
            "kalipharma\n",
            "nesb\n",
            "auto-safety\n",
            "c.j.b.\n",
            "pramual\n",
            "biondi-santi\n",
            "1.5755\n",
            "sacramento-based\n",
            "home-market\n",
            "test-prep\n",
            "morale-damaging\n",
            "video-viewing\n",
            "machine-gun-toting\n",
            "veselich\n",
            "1\\/10th\n",
            "three-lawyer\n",
            "352.7\n",
            "front-seat\n",
            "rapanelli\n",
            "8300s\n",
            "hallwood\n",
            "236.74\n",
            "school-research\n",
            "hummerstone\n",
            "monchecourt\n",
            "yeargin\n",
            "product-design\n",
            "dollar-yen\n",
            "satrum\n",
            "vitulli\n",
            "c-90\n",
            "re-thought\n",
            "ctbs\n",
            "high-balance\n",
            "government-certified\n",
            "62%-owned\n",
            "when-issued\n",
            "food-shop\n",
            "muscolina\n",
            "highest-pitched\n",
            "-lcb-\n",
            "wine-buying\n",
            "equal-opportunity\n",
            "pennview\n",
            "7\\/8\n",
            "271,124\n",
            "asset-sale\n",
            "sell-offs\n",
            "-rrb-\n",
            "high-rate\n",
            "tiphook\n",
            "foreign-stock\n",
            "computer-system-design\n",
            "stock-manipulation\n",
            "superpremiums\n",
            "senate-house\n",
            "test-practice\n",
            "ingersoll-rand\n",
            "besuboru\n",
            "sub-markets\n",
            "rexinger\n",
            "142.85\n",
            "70-a-share\n",
            "62.625\n",
            "lafite-rothschild\n",
            "sometimes-tawdry\n",
            "trettien\n",
            "-lrb-\n",
            "90-cent-an-hour\n",
            "automotive-lighting\n",
            "pro-forma\n",
            "bridgestone\\/firestone\n",
            "rubinfien\n",
            "derel\n",
            "corton-charlemagne\n",
            "moleculon\n",
            "chinchon\n",
            "thin-lipped\n",
            "direct-investment\n",
            "yen-support\n",
            "gingl\n",
            "3,288,453\n",
            "summer\\/winter\n",
            "electric-utility\n",
            "nissho-iwai\n",
            "bell-ringer\n",
            "705.6\n",
            "intellectual-property\n",
            "230-215\n",
            "456.64\n",
            "uzi-model\n",
            "side-crash\n",
            "custom-chip\n",
            "lap-shoulder\n",
            "small-company\n",
            "3057\n",
            "bellringers\n",
            "abortion-related\n",
            "361,376\n",
            "anti-china\n",
            "savings-and-loan\n",
            "futures-related\n",
            "macmillan\\/mcgraw-hill\n",
            "odd-sounding\n",
            "nagymaros\n",
            "iran\\/contra\n",
            "roof-crush\n",
            "delwin\n",
            "creator's\n",
            "2,303,328\n",
            "marketing-communications\n",
            "univest\n",
            "nekoosa\n",
            "program-trading\n",
            "car-safety\n",
            "state-supervised\n",
            "100,980\n",
            "drag-down\n",
            "erbamont\n",
            "pathlogy\n",
            "post-hearing\n",
            "69-point\n",
            "purepac\n",
            "church-goers\n",
            "weisfield\n",
            "374.19\n",
            "big-ticket\n",
            "coche-dury\n",
            "teacher-cadet\n",
            "84-month\n",
            "4.898\n",
            "times-stock\n",
            "sometimes-exhausting\n",
            "buttoned-down\n",
            "two-sevenths\n",
            "95,142\n",
            "war-rationed\n",
            "forest-products\n",
            "ghkm\n",
            "low-ball\n",
            "college-bowl\n",
            "tissue-transplant\n",
            "top-yielding\n",
            "less-serious\n",
            "northy\n",
            "aslacton\n",
            "pre-1933\n",
            "crocidolite\n",
            "wheel-loader\n",
            "search-and-seizure\n",
            "romanee-conti\n",
            "415.6\n",
            "training-wage\n",
            "374.20\n",
            "449.04\n",
            "13,056\n",
            "subminimum\n",
            "social-studies\n",
            "dead-eyed\n",
            "14,821\n",
            "ft-se\n",
            "fetal-tissue\n",
            "life-insurance\n",
            "money-center\n",
            "4,393,237\n",
            "deposits-a\n",
            "unfair-trade\n",
            "non-biodegradable\n",
            "macmillan\\/mcgraw\n",
            "143.80\n",
            "16,072\n",
            "38.375\n",
            "82,389\n",
            "akerfeldt\n",
            "382-37\n",
            "identity-management\n",
            "school-district\n",
            "six-packs\n",
            "shirt-sleeved\n",
            "sanderoff\n",
            "language-housekeeper\n",
            "1\\/2\n",
            "1.8415\n",
            "change-ringing\n",
            "yen-denominated\n",
            "chemplus\n",
            "stock-index\n",
            "bald-faced\n",
            "nipponese\n",
            "water-authority\n",
            "mehrens\n",
            "pattenden\n",
            "polyproplene\n",
            "wheeland\n",
            "737.5\n",
            "circuit-breaker\n",
            "security-type\n",
            "telephone-information\n",
            "chong-sik\n",
            "rate-sensitive\n",
            "six-bottle\n",
            "old-house\n",
            "limited-partnership\n",
            "junk-bond\n",
            "drobnick\n",
            "retin-a\n",
            "sport-utility\n",
            "school-board\n",
            "co-developers\n",
            "nearly-30\n",
            "wfrr\n",
            "family-planning\n",
            "jerritts\n",
            "twin-jet\n",
            "glenham\n",
            "durable-goods\n",
            "index-options\n",
            "ratners\n",
            "malizia\n",
            "497.34\n",
            "trading-company\n",
            "school-improvement\n",
            "norwick\n",
            "beer-belly\n",
            "incentive-bonus\n",
            "278.7\n",
            "37-a-share\n",
            "student-test\n",
            "light-truck\n",
            "5.276\n",
            "three-sevenths\n",
            "stirlen\n",
            "integra-a\n",
            "subskill\n",
            "detective-story\n",
            "page-one\n",
            "mininum-wage\n",
            "jalaalwalikraam\n",
            "energy-services\n",
            "greenmailer\n",
            "money-fund\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "Here we replace bracket value with their symbols: \n",
        "```\n",
        "  -lrb- and -lcb-   -->  ( \n",
        "  -rrb- and -rcb-   -->  )\n",
        "```\n",
        "\n",
        "In addition, all the rational numbers will be replaced with the placeholder #number#, as long as the floating point numbers.\n",
        "Note that rational numbers, instead of being like 3/4, are written as 3\\/4. The cause is that symbol \"/\" is represented using \"\\/\", as this happens also in other words that are notrational ones\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b5RNdNYYHYwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(content_list: List[str]) -> List[str]:\n",
        "    placeholder = \"#number#\"\n",
        "    re_slashes = re.compile('\\\\\\/')  #pattern \\/ \n",
        "    re_rational = re.compile('\\d+\\/\\d+')  #pattern rational number (e.g. 1/5)\n",
        "    re_number = re.compile('[+-]?(\\d*[.])\\d+')  #pattern decimal number (e.g. 3.14)\n",
        "    re_left_bracket = re.compile('(-lrb-)|(-lcb-)')  #pattern left bracket\n",
        "    re_right_bracket = re.compile('(-rrb-)|(-rcb-)')  #pattern right bracket\n",
        "    re_slashed_words = re.compile(\"(\\w*)\\/(\\w*)\")  #a slash separating words will be replaced with a dash, following the trend of the dataset, where composed words are in the form word-word\n",
        "\n",
        "    content_list_preprocessed = [content.lower() for content in content_list]\n",
        "    content_list_preprocessed = [re_slashes.sub(\"/\", content) for content in content_list_preprocessed]\n",
        "    content_list_preprocessed = [re_left_bracket.sub(\"(\", content) for content in content_list_preprocessed]\n",
        "    content_list_preprocessed = [re_right_bracket.sub(\")\", content) for content in content_list_preprocessed]\n",
        "    content_list_preprocessed = [placeholder if re.match(re_rational, content) else content for content in content_list_preprocessed]\n",
        "    content_list_preprocessed = [placeholder if re.match(re_number, content) else content for content in content_list_preprocessed]\n",
        "    content_list_preprocessed = [content.replace(\"/\", \"-\") if re.match(re_slashed_words, content) else content for content in content_list_preprocessed]\n",
        "\n",
        "    return content_list_preprocessed\n"
      ],
      "metadata": {
        "id": "d9H61yyiHYbT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the training dataset"
      ],
      "metadata": {
        "id": "OzNl0nSGdEwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_preprocessed = x[\"train\"].apply(preprocessing)"
      ],
      "metadata": {
        "id": "GyjnI2JZbADI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the new vocabulary after preprocessing"
      ],
      "metadata": {
        "id": "M-Vw8s63garq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_vocabulary_preprocessed = build_vocabulary(x_train_preprocessed)\n",
        "print()\n",
        "print(f'[Debug] train vocabulary size after preprocessing: {len(train_vocabulary_preprocessed)}')\n",
        "\n",
        "OOV1_preprocessed = check_OOV_terms(glove_emb_model.vocab.keys(), train_vocabulary_preprocessed)\n",
        "OOV1_preprocessed_percentage = float(len(OOV1_preprocessed)) * 100 / len(train_vocabulary_preprocessed)\n",
        "print(f\"Total OOV terms: {len(OOV1_preprocessed)} ({OOV1_preprocessed_percentage:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcxlVwQFgaRr",
        "outputId": "12682b9e-7e2f-400d-969f-0a63ef88615f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1963/1963 [00:00<00:00, 2377.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] train vocabulary size after preprocessing: 7214\n",
            "Total OOV terms: 318 (4.41%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that the number of OOV words has plummetted with respect to the the non preprocessed data. Similarly to train data, we apply preprocessing to validation and test splits."
      ],
      "metadata": {
        "id": "j5OLPRGc6svj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_pre = {\"train\": x_train_preprocessed,\n",
        "        \"val\": x[\"val\"].apply(preprocessing),\n",
        "        \"test\": x[\"test\"].apply(preprocessing)}"
      ],
      "metadata": {
        "id": "MEec69AT_KEw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocabulary creation and mapping\n",
        "Since we want to work with numerical data only, we will mapp words and pos (labels) to numbers. "
      ],
      "metadata": {
        "id": "EElOhl9F-x9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Methods to create mapping\n",
        "\n",
        "#adds oov words at the end of vocabulary\n",
        "def extend_vocabulary(word_to_idx_original: Dict[str, int],\n",
        "                      words_to_add) -> Tuple[Dict[str, int],Dict[int, str]]:\n",
        "  \"\"\"\n",
        "    Given mapping between word and indeces, adds new words.\n",
        "\n",
        "    :param word_to_idx_original: dictionary with key=word and value=index to which the word is mapped\n",
        "    :return:\n",
        "      - word_to_idx_extended: word_to_idx with new words\n",
        "      - idx_to_word_extended: swapped version of word_to_idx_extended (keys and values are swapped)\n",
        "  \"\"\"\n",
        "  word_to_idx_extended = copy.deepcopy(word_to_idx_original)  #deep copy is needed, otherwise python does not create a copy but only a reference to the already existing object, thus reflecting changes on both\n",
        "  idx = len(word_to_idx_extended.keys())\n",
        "  if idx == 0: \n",
        "    idx = 1  #position 0 is reserved\n",
        "\n",
        "  for sentence in words_to_add:\n",
        "      for token in sentence:\n",
        "          if token not in word_to_idx_extended:\n",
        "              word_to_idx_extended[token] = idx \n",
        "              idx += 1\n",
        "  idx_to_word_extended = {v: k for k, v in word_to_idx_extended.items()}\n",
        "\n",
        "  return word_to_idx_extended, idx_to_word_extended\n",
        "\n",
        "def encode_into_numbers(sentences: List[str],\n",
        "                        word_to_idx_mapping: Dict[str, int]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Return a list of sequences encoded into integers following the mapping of the vocabulary\n",
        "    \"\"\"\n",
        "    encoded_data = [[word_to_idx_mapping[token] for token in sentence] for sentence in sentences]\n",
        " \n",
        "    return encoded_data\n",
        "\n",
        "def decode_into_words(encoded_sentences: List[str],\n",
        "                        idx_to_word_mapping: Dict[int,str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Return a list of sequences decoded back to words following the (reverse) mapping of the vocabulary\n",
        "    \"\"\"\n",
        "    decoded_data = [[idx_to_word_mapping[index] for index in sentence] for sentence in encoded_sentences]\n",
        " \n",
        "    return decoded_data\n"
      ],
      "metadata": {
        "id": "_j91cRBqAN1x"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding the words"
      ],
      "metadata": {
        "id": "HV32wJ90RamP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating vocabulary mapping for the words in the data set\n",
        "#Note that they are incremental, this means that the val vocabulary includes the rain one, and the test one inlcudes train and val ones\n",
        "#This has been made according to the guidelines on the construction of V1, V2, V3, V4. \n",
        "#All in all, the complete vocabulary is the one with _test suffix\n",
        "#In the embedding section the intermediate vocabularies will be used according to what they contain. For example, to compute the embedding matrix on the train set, we will use word_to_idx_train, while for validation word_to_idx_val\n",
        "word_to_idx_train, idx_to_word_train = extend_vocabulary({}, [glove_emb_model.vocab.keys()] + x_pre[\"train\"].tolist())\n",
        "print(\"Train vocabulary size: \", len(word_to_idx_train))\n",
        "word_to_idx_val, idx_to_word_val = extend_vocabulary(word_to_idx_train, x_pre[\"val\"].tolist())\n",
        "print(\"Val vocabulary size: \", len(word_to_idx_val))\n",
        "word_to_idx_test, idx_to_word_test = extend_vocabulary(word_to_idx_val, x_pre[\"test\"].tolist())\n",
        "print(\"Test vocabulary size: \", len(word_to_idx_test))\n",
        "\n",
        "#encoding the data set\n",
        "\n",
        "x_enc = {\"train\": encode_into_numbers(x_pre[\"train\"].tolist(), word_to_idx_train),\n",
        "        \"val\": encode_into_numbers(x_pre[\"val\"].tolist(), word_to_idx_val),\n",
        "        \"test\": encode_into_numbers(x_pre[\"test\"].tolist(), word_to_idx_test)}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbeQJNAlEzl2",
        "outputId": "99b84b1a-6350-49ba-f7a1-234cc19f895e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train vocabulary size:  400318\n",
            "Val vocabulary size:  400475\n",
            "Test vocabulary size:  400571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding the labels (pos)"
      ],
      "metadata": {
        "id": "qGsgMfcmRUIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating vocabulary mapping for the labels in the whole dataset\n",
        "label_to_idx, idx_to_label = extend_vocabulary({},  y[\"train\"].tolist() + y[\"val\"].tolist() + y[\"test\"].tolist())\n",
        "\n",
        "y_enc = {\"train\": encode_into_numbers(y[\"train\"].tolist(), label_to_idx),\n",
        "        \"val\": encode_into_numbers(y[\"val\"].tolist(), label_to_idx),\n",
        "        \"test\": encode_into_numbers(y[\"test\"].tolist(), label_to_idx)}\n",
        "\n",
        "number_pos = len(label_to_idx)\n",
        "print(f\"In the dataset there are {number_pos} distinct POS\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PzlKA-eHJBr",
        "outputId": "4412ae5b-0255-4e72-959c-1ffa0053e8ae"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the dataset there are 45 distinct POS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_enc[\"train\"][0])\n",
        "print(decode_into_words([x_enc[\"train\"][0]], idx_to_word_test))\n",
        "print(df_dict[\"train\"][\"sentence\"].iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwaxJlw3AV0",
        "outputId": "15d5ab3c-e22f-4cd6-bb59-be333937df7b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5030, 400001, 2, 4979, 83, 168, 2, 44, 1430, 1, 535, 20, 8, 128565, 370, 2344, 1264, 3]\n",
            "[['pierre', 'vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '29', '.']]\n",
            "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode_into_words([y_enc[\"train\"][0]], idx_to_label))\n",
        "print(df_dict[\"train\"][\"labels\"].iloc[0])\n",
        "print(y_enc[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3G3-Pbx3ZmJ",
        "outputId": "feba7875-92c7-4896-8d87-aa02cd278a05"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']]\n",
            "['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']\n",
            "[1, 1, 2, 3, 4, 5, 2, 6, 7, 8, 9, 10, 8, 5, 9, 1, 3, 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding matrix"
      ],
      "metadata": {
        "id": "mAv5AHB_REM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dashed_embeddings(embedding_model, word):\n",
        "  if \"-\" in word:\n",
        "    words_split = word.split(\"-\")\n",
        "    words_split.sort(key=len)  #getting the encoding of compound words starting from the longest one\n",
        "    for word_piece in words_split:\n",
        "      try:\n",
        "        return embedding_model[word]  #if a word is found, assign its embedding to the matrix element\n",
        "      except:\n",
        "        pass  #if a word is not found, do nothing\n",
        "  return None\n",
        "\n",
        "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                           embedding_dimension: int,\n",
        "                           word_to_idx: Dict[str, int]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param embedding_dimension: \n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((len(word_to_idx)+1, embedding_dimension), dtype=np.float32)\n",
        "    \n",
        "    #adding all GloVe vocabularies embeddings\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "      if word in embedding_model:\n",
        "          embedding_matrix[idx] = embedding_model[word]\n",
        "      else: \n",
        "          dashed_embedding = get_dashed_embeddings(embedding_model, word)\n",
        "          if dashed_embedding is None: #it means that word has no dash or all its subwords are oov\n",
        "              dashed_embedding = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "          embedding_matrix[idx] = dashed_embedding\n",
        " \n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "#This functions adds the embedding of OOV words to the embedding matrix. Note tht it directly tries to find an embedding for dashed words and if none is retrieved it uses a uniform random distribution\n",
        "def extend_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                            embedding_matrix: np.ndarray,\n",
        "                            word_to_idx: Dict[str, int]) -> np.ndarray:\n",
        "\n",
        "    oov_terms = [key for key, idx in word_to_idx.items() if idx >= embedding_matrix.shape[0]] #all the terms with mapped to an index gretaer than the vocabulary size (number of rows) are not in the embedding matrix \n",
        "    oov_embedding_matrix = np.zeros((len(oov_terms), embedding_matrix.shape[1]), dtype=np.float32)\n",
        "    \n",
        "    for idx, oov in enumerate(oov_terms):\n",
        "        dashed_embedding = get_dashed_embeddings(embedding_model, oov)\n",
        "        if dashed_embedding is None: #it means that word has no dash or all its subwords are oov\n",
        "            dashed_embedding = np.random.uniform(low=-0.05, high=0.05, size=embedding_matrix.shape[1])\n",
        "\n",
        "        oov_embedding_matrix[idx] = dashed_embedding\n",
        "\n",
        "    return np.concatenate([embedding_matrix, oov_embedding_matrix], axis=0)"
      ],
      "metadata": {
        "id": "QGCi8pguRH-5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = build_embedding_matrix(glove_emb_model, \n",
        "                                          EMBEDDING_DIMENSION,\n",
        "                                          word_to_idx_train)\n",
        "print(embedding_matrix.shape)\n",
        "\n",
        "embedding_matrix = extend_embedding_matrix(glove_emb_model, \n",
        "                                          embedding_matrix,\n",
        "                                          word_to_idx_val)\n",
        "print(embedding_matrix.shape)\n",
        "\n",
        "embedding_matrix = extend_embedding_matrix(glove_emb_model, \n",
        "                                          embedding_matrix,\n",
        "                                          word_to_idx_test)\n",
        "print(embedding_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERNzAqnMELNB",
        "outputId": "e917199e-b87c-4065-9a08-a051b9325b55"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400318/400318 [00:00<00:00, 448112.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(400319, 50)\n",
            "(400475, 50)\n",
            "(400571, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(idx_to_word_train[400001])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nH2_yE1B5Lc",
        "outputId": "c1cbec5d-c933-4bb5-d68c-d797f485eef8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vinken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_matrix[400001])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z88wYmcqBftt",
        "outputId": "cd07159e-fe41-445d-e883-0a6874cdd339"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.03338091  0.04008736  0.01017872 -0.04682402  0.0133119   0.0451397\n",
            " -0.0420833   0.00824561  0.04154786 -0.04903048  0.01309129 -0.00250977\n",
            " -0.01530107  0.01275365 -0.04472979  0.0253063  -0.04955205 -0.01930643\n",
            "  0.00039586 -0.01229777 -0.01925241  0.03308351  0.04348326  0.03737127\n",
            "  0.00224313  0.03841642  0.04208616 -0.00907616 -0.02165541 -0.04017348\n",
            " -0.00209081  0.04462969  0.00609075  0.01675621 -0.03871162 -0.03564803\n",
            " -0.00355267  0.04512648 -0.01392154  0.0377484  -0.03797114  0.01170172\n",
            "  0.0100613   0.03598747 -0.04966833 -0.03100683 -0.02803124  0.04499661\n",
            " -0.03463063 -0.04630376]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence length standardization \n",
        "Every sentence must have the same length, otherwise we would habe different input sizes"
      ],
      "metadata": {
        "id": "2HIZO1d8SPFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length_dict = {\"train\": len(max(x_enc[\"train\"], key=len)),\n",
        "                   \"val\": len(max(x_enc[\"val\"], key=len)),\n",
        "                   \"test\": len(max(x_enc[\"test\"], key=len))}\n",
        "\n",
        "number_pos = len(label_to_idx) + 1 #to add the padding\n",
        "x_st, y_st, y_cat = {}, {}, {}\n",
        "\n",
        "for key in max_length_dict.keys():\n",
        "    x_st[key] = pad_sequences(x_enc[key], maxlen=max_length_dict[key], padding='post')\n",
        "    y_st[key] = pad_sequences(y_enc[key], maxlen=max_length_dict[key], padding='post')\n",
        "    y_cat[key] = to_categorical(y_st[key], num_classes=number_pos)"
      ],
      "metadata": {
        "id": "PHtxPN12Xu12"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_cat[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ4rzkCCCrDQ",
        "outputId": "eb0f9669-a42c-47ff-8423-b115cfe37259"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "yoFjfVeiiN4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(layer_list, model_name):\n",
        "    input = Input(shape=(None,), dtype=\"int32\")\n",
        "    t = layer_list[0](input)\n",
        "    for layer in layer_list[1:]:\n",
        "        t = layer(t)\n",
        "    return Model(inputs=input, outputs=t, name=model_name)"
      ],
      "metadata": {
        "id": "mJuzjdbijeLU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {}\n",
        "history = {}\n",
        "batch_size = 8\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "51i4hAotnH_s"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline model"
      ],
      "metadata": {
        "id": "dzdywqEviQla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_layers = [\n",
        "    Embedding(input_dim=embedding_matrix.shape[0], \n",
        "              output_dim=embedding_matrix.shape[1],\n",
        "              mask_zero=True, \n",
        "              weights=[embedding_matrix], \n",
        "              trainable=False),\n",
        "    Bidirectional(LSTM(units=100, return_sequences=True)),\n",
        "    Dense(number_pos, activation='softmax')\n",
        "]\n",
        "\n",
        "models[\"baseline\"] = build_model(baseline_layers, \"baseline\")"
      ],
      "metadata": {
        "id": "AzKme1uQiYRC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models[\"baseline\"].summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRTKBM2dnVYW",
        "outputId": "681f75bd-378d-4a9e-875d-0d58c8800cf1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 50)          20028550  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, None, 200)        120800    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 46)          9246      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,158,596\n",
            "Trainable params: 130,046\n",
            "Non-trainable params: 20,028,550\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models['baseline'].compile(optimizer = Adam(), \n",
        "                           loss = CategoricalCrossentropy(), \n",
        "                           metrics = [Accuracy()])\n",
        "\n",
        "history['baseline'] = models['baseline'].fit(x=x_st[\"train\"], \n",
        "                   y=y_cat[\"train\"], \n",
        "                   batch_size=batch_size, \n",
        "                   epochs=epochs, \n",
        "                   validation_data=(x_st[\"val\"], y_cat[\"val\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E69SDCkmlv1h",
        "outputId": "17cd9ed0-1d5f-4bc3-de1e-abb94d344e2d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "246/246 [==============================] - 18s 29ms/step - loss: 0.1855 - accuracy: 0.0000e+00 - val_loss: 0.3451 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0886 - accuracy: 0.0000e+00 - val_loss: 0.2460 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "  1/246 [..............................] - ETA: 7s - loss: 0.0870 - accuracy: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-50-18dbecacc770>\", line 9, in <module>\n",
            "    validation_data=(x_st[\"val\"], y_cat[\"val\"]))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1414, in fit\n",
            "    callbacks.on_train_batch_end(end_step, logs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\", line 438, in on_train_batch_end\n",
            "    self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\", line 297, in _call_batch_hook\n",
            "    self._call_batch_end_hook(mode, batch, logs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\", line 318, in _call_batch_end_hook\n",
            "    self._call_batch_hook_helper(hook_name, batch, logs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\", line 356, in _call_batch_hook_helper\n",
            "    hook(batch, logs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\", line 1034, in on_train_batch_end\n",
            "    self._batch_update_progbar(batch, logs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\", line 1106, in _batch_update_progbar\n",
            "    logs = tf_utils.sync_to_numpy_or_python_type(logs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\", line 607, in sync_to_numpy_or_python_type\n",
            "    return tf.nest.map_structure(_to_single_numpy_or_python_type, tensors)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\", line 916, in map_structure\n",
            "    structure[0], [func(*x) for x in entries],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\", line 916, in <listcomp>\n",
            "    structure[0], [func(*x) for x in entries],\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\", line 601, in _to_single_numpy_or_python_type\n",
            "    t = t.numpy()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 1159, in numpy\n",
            "    maybe_arr = self._numpy()  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 1125, in _numpy\n",
            "    return self._numpy_internal()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 742, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 395, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
            "    if not islink(newpath):\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 171, in islink\n",
            "    st = os.lstat(path)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X: \", x_enc[\"train\"][0])\n",
        "print(\"X Encoded: \", x_st[\"train\"][0], '\\n\\n', 100*'=', '\\n')\n",
        "print(\"Y: \", y_enc[\"train\"][0])\n",
        "print(\"Y Encoded:\", y_st[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e64MwWdm0X_K",
        "outputId": "303206cb-00dd-4313-a60f-f5e3a3c5afd9"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:  [4966, 97765, 2, 400001, 83, 168, 6, 158, 91, 6, 277, 1478, 942, 4, 6545, 29, 1912, 28, 96, 732, 2, 6, 5842, 33267, 2231, 2, 400001, 2, 664, 4, 44321, 2129, 1019, 2, 36, 1021, 3199, 4, 38, 1494, 3720, 6, 11262, 17564, 2854, 3]\n",
            "X Encoded:  [  4966  97765      2 400001     83    168      6    158     91      6\n",
            "    277   1478    942      4   6545     29   1912     28     96    732\n",
            "      2      6   5842  33267   2231      2 400001      2    664      4\n",
            "  44321   2129   1019      2     36   1021   3199      4     38   1494\n",
            "   3720      6  11262  17564   2854      3      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0] \n",
            "\n",
            " ==================================================================================================== \n",
            "\n",
            "Y:  [1, 1, 2, 3, 4, 5, 6, 5, 7, 6, 7, 8, 7, 9, 10, 11, 1, 12, 1, 1, 2, 6, 1, 1, 1, 2, 3, 2, 7, 9, 1, 1, 1, 2, 13, 14, 4, 9, 15, 7, 4, 6, 4, 7, 7, 16]\n",
            "Y Encoded: [ 1  1  2  3  4  5  6  5  7  6  7  8  7  9 10 11  1 12  1  1  2  6  1  1\n",
            "  1  2  3  2  7  9  1  1  1  2 13 14  4  9 15  7  4  6  4  7  7 16  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZQ8e1t7MzP2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "YMWAFhZnDCDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(input_dim, \n",
        "                 num_classes, \n",
        "                 embeddings, \n",
        "                 name = 'baseline',\n",
        "                 use_GRU = False, \n",
        "                 additional_LSTM = False, \n",
        "                 additional_dense = False,\n",
        "                 units=100,\n",
        "                 dropout=0.0):\n",
        "  \"\"\"\n",
        "  Create a Keras model for the POS-tagging task\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  input_dim: int\n",
        "      The vocabulary size\n",
        "  num_classes: int\n",
        "      The number of different POS-tags (+ a class for the padding value) to be predicted \n",
        "  embeddings: numpy.ndarray\n",
        "      Matrix containing the pre-trained embeddings \n",
        "  name: str\n",
        "      The name of the model\n",
        "  use_GRU: bool\n",
        "      If True, use a BiGRU layer instead of the BiLSTM\n",
        "  additional_LSTM: bool\n",
        "      If True, use an additional BiLSTM layer right after the default one\n",
        "  additional_dense: bool\n",
        "      If True, adds a FC layer before the classifier\n",
        "  units: int\n",
        "      The hidden state's dimension of the RNNs\n",
        "  dropout: float\n",
        "      Define the drop rate of the Dropout layers. By default the dropout is disabled\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tensorflow.keras.models.Model\n",
        "      The POS-tagging model\n",
        "      \n",
        "  \"\"\"\n",
        "  inputs = Input(shape=(None,), dtype=\"int32\")\n",
        "  x = Embedding(\n",
        "      input_dim=input_dim, \n",
        "      output_dim=embeddings.shape[1],\n",
        "      mask_zero=True, \n",
        "      weights=[embeddings], \n",
        "      trainable=False)(inputs)\n",
        "  if dropout:\n",
        "    x = Dropout(dropout)(x)\n",
        "  if use_GRU:\n",
        "    rnn = GRU(units=units, return_sequences=True)\n",
        "  else:\n",
        "    rnn = LSTM(units=units, return_sequences=True, dropout=dropout)\n",
        "  x = Bidirectional(rnn)(x)\n",
        "  if additional_LSTM:\n",
        "    x = LSTM(units=units, return_sequences=True)(x)\n",
        "  if additional_dense:\n",
        "    x = Dense(100, activation='relu')(x)\n",
        "  x = Dense(num_classes, activation='softmax')(x)\n",
        "  return Model(inputs, x, name=name)"
      ],
      "metadata": {
        "id": "tGR8JYrfDTEb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "epochs = 100\n",
        "models = {}\n",
        "histories = {}"
      ],
      "metadata": {
        "id": "AUGoq9iDDaGn"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models['baseline'] = get_model(embedding_matrix.shape[0], number_pos, embedding_matrix)\n",
        "models['baseline'].summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMBjvmWLDbch",
        "outputId": "bbba4d4e-f6ee-4597-a609-35a6ed12cad2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"baseline\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 50)          20028550  \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, None, 200)        120800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, None, 46)          9246      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,158,596\n",
            "Trainable params: 130,046\n",
            "Non-trainable params: 20,028,550\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models['baseline'].compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "histories['baseline'] = models['baseline'].fit(x=x_st[\"train\"], \n",
        "                   y=y_cat[\"train\"], \n",
        "                   batch_size=batch_size, epochs=epochs, \n",
        "                   validation_data=(x_st[\"val\"], y_cat[\"val\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9jMME5DDedp",
        "outputId": "3db09e3c-94cf-4ae2-b995-9f23a330c3ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "246/246 [==============================] - 14s 27ms/step - loss: 0.1838 - accuracy: 0.5059 - val_loss: 0.3403 - val_accuracy: 0.6904\n",
            "Epoch 2/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0875 - accuracy: 0.7557 - val_loss: 0.2435 - val_accuracy: 0.7637\n",
            "Epoch 3/100\n",
            "246/246 [==============================] - 5s 18ms/step - loss: 0.0673 - accuracy: 0.8057 - val_loss: 0.2084 - val_accuracy: 0.7956\n",
            "Epoch 4/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0571 - accuracy: 0.8324 - val_loss: 0.1858 - val_accuracy: 0.8166\n",
            "Epoch 5/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0501 - accuracy: 0.8521 - val_loss: 0.1700 - val_accuracy: 0.8321\n",
            "Epoch 6/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0445 - accuracy: 0.8689 - val_loss: 0.1586 - val_accuracy: 0.8419\n",
            "Epoch 7/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0399 - accuracy: 0.8834 - val_loss: 0.1490 - val_accuracy: 0.8514\n",
            "Epoch 8/100\n",
            "246/246 [==============================] - 5s 22ms/step - loss: 0.0360 - accuracy: 0.8951 - val_loss: 0.1447 - val_accuracy: 0.8527\n",
            "Epoch 9/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0328 - accuracy: 0.9046 - val_loss: 0.1369 - val_accuracy: 0.8615\n",
            "Epoch 10/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0297 - accuracy: 0.9138 - val_loss: 0.1340 - val_accuracy: 0.8616\n",
            "Epoch 11/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0270 - accuracy: 0.9222 - val_loss: 0.1298 - val_accuracy: 0.8677\n",
            "Epoch 12/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0246 - accuracy: 0.9310 - val_loss: 0.1250 - val_accuracy: 0.8733\n",
            "Epoch 13/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0223 - accuracy: 0.9378 - val_loss: 0.1238 - val_accuracy: 0.8742\n",
            "Epoch 14/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0203 - accuracy: 0.9449 - val_loss: 0.1229 - val_accuracy: 0.8737\n",
            "Epoch 15/100\n",
            "246/246 [==============================] - 4s 18ms/step - loss: 0.0184 - accuracy: 0.9505 - val_loss: 0.1224 - val_accuracy: 0.8724\n",
            "Epoch 16/100\n",
            "246/246 [==============================] - 5s 21ms/step - loss: 0.0166 - accuracy: 0.9565 - val_loss: 0.1218 - val_accuracy: 0.8755\n",
            "Epoch 17/100\n",
            "122/246 [=============>................] - ETA: 1s - loss: 0.0147 - accuracy: 0.9642"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # The folllowing section is only for reference, as it has been completely rewritten"
      ],
      "metadata": {
        "id": "XCpn4_SskmsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OOV handling"
      ],
      "metadata": {
        "id": "AOm0QZVUbpnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this function extends the current embedding matrix with the embeddings of the oov terms\n",
        "def extend_embedding_matrix(embedding_matrix: np.ndarray,\n",
        "                            oov_terms: List[str]) -> np.ndarray:\n",
        "\n",
        "    oov_embedding_matrix = np.zeros((len(oov_terms), embedding_matrix.shape[1]), dtype=np.float32)\n",
        "    for idx, oov in enumerate(oov_terms):\n",
        "        embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_matrix.shape[1])\n",
        "        oov_embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    new_embedding_matrix = np.concatenate([embedding_matrix, oov_embedding_matrix])\n",
        "\n",
        "    return new_embedding_matrix"
      ],
      "metadata": {
        "id": "ClM447bcOIKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check OOV1 (oov in the train set) and add OOV1 embeddings to the matrix"
      ],
      "metadata": {
        "id": "i5zlr_imBZMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OOV1 = check_OOV_terms(set(glove_emb_model.vocab.keys()), word_listing_train)"
      ],
      "metadata": {
        "id": "yxWpJouSC3ZA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "6c1d27d6-93ad-43fb-c279-ee1b2b973342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-49932e0aeb81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mOOV1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_OOV_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_emb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_listing_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'word_listing_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oov_percentage = float(len(OOV1)) * 100 / len(word_listing_train)\n",
        "print(f\"Total OOV terms: {len(OOV1)} ({oov_percentage:.2f}%)\")"
      ],
      "metadata": {
        "id": "8j0KBHdXDTf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding OOV1 embeddings to the matrix"
      ],
      "metadata": {
        "id": "mPpBxdhsVolP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = extend_embedding_matrix(embedding_matrix, OOV1)\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
      ],
      "metadata": {
        "id": "xqSqEagMTGK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check OOV2 (oov in the validation set) and add OOV2 embeddings to the matrix"
      ],
      "metadata": {
        "id": "tUY_G3VFZ_Pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OOV2 = check_OOV_terms(set(glove_emb_model.vocab.keys()).union(set(OOV1)), word_listing_val)"
      ],
      "metadata": {
        "id": "icE_iBCjZ_P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov2_percentage = float(len(OOV2)) * 100 / len(word_listing_val)\n",
        "print(f\"Total OOV terms: {len(OOV2)} ({oov2_percentage:.2f}%)\")"
      ],
      "metadata": {
        "id": "BQeFn7fQZ_P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Adding OOV2 embeddings to the matrix\n",
        "\n",
        "This cell is useless because OOV2 is empty"
      ],
      "metadata": {
        "id": "8j5sjYhmZ_P3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = extend_embedding_matrix(embedding_matrix, OOV2)\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
      ],
      "metadata": {
        "id": "OCwm_WIwZ_P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check OOV3 (oov in the test set) and add OOV3 embeddings to the matrix"
      ],
      "metadata": {
        "id": "GYUIRZW9bPXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OOV3 = check_OOV_terms(set(glove_emb_model.vocab.keys()).union(set(OOV1)).union(set(OOV2)), word_listing_test)"
      ],
      "metadata": {
        "id": "fk8W0p48bPXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov3_percentage = float(len(OOV3)) * 100 / len(word_listing_test)\n",
        "print(f\"Total OOV terms: {len(OOV3)} ({oov3_percentage:.2f}%)\")"
      ],
      "metadata": {
        "id": "8rVI78EFbPXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Adding OOV3 embeddings to the matrix\n",
        "This cell is useless too, as no oov term is present in the test set"
      ],
      "metadata": {
        "id": "WkEY-YJ6bPXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = extend_embedding_matrix(embedding_matrix, OOV3)\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
      ],
      "metadata": {
        "id": "BInrt9tJbPXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build embedding matrix"
      ],
      "metadata": {
        "id": "M-T50oWZXHTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting from GloVe matrix"
      ],
      "metadata": {
        "id": "uXu5FElyFHXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                           embedding_dimension: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param embedding_dimension: \n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((len(embedding_model.index2word), embedding_dimension), dtype=np.float32)\n",
        "    \n",
        "    #adding all GloVe vocabularies embeddings\n",
        "    for idx, word in enumerate(embedding_model.index2word):\n",
        "         embedding_vector = embedding_model[word]\n",
        "         embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "Etd4xRC2Qi15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vocab_size = len(glove_emb_model.index2word) + len(OOV1)\n",
        "embedding_matrix = build_embedding_matrix(glove_emb_model, EMBEDDING_DIMENSION)\n",
        "print()\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
      ],
      "metadata": {
        "id": "mmzp9NYFQ7kD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}